{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef52373",
   "metadata": {},
   "source": [
    "### Content-Based Filtering: Overview and Motivation\n",
    "\n",
    "**Content-based filtering** is a recommendation approach that suggests items to a user based on the **similarity between item attributes and the user’s profile**. Each item (e.g., video, product, article) is described by a set of features — such as categories, metadata, or text — and each user is represented by a profile built from the features of the items they liked or interacted with.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Choose Content-Based Filtering?\n",
    "\n",
    "- **No need for user overlap**: Unlike collaborative filtering, it works well even when users don't share many common interactions.\n",
    "- **Cold-start friendly for users**: New users can receive recommendations as soon as they interact with a few items.\n",
    "- **Explainability**: Recommendations are based on content similarity, making it easier to interpret why an item was recommended.\n",
    "- **Control and customization**: Designers can influence recommendations by choosing or engineering relevant item features (e.g., engagement stats, captions, categories).\n",
    "- **Independence from other users**: User preferences are modeled individually, which avoids popularity bias and herd effects.\n",
    "\n",
    "---\n",
    "\n",
    "#### Limitations (to be addressed in hybrid systems)\n",
    "\n",
    "- It may lead to **narrow recommendations** (serendipity is limited).\n",
    "- It doesn't leverage the collective behavior of similar users.\n",
    "- It can underperform if **content features are not expressive enough**.\n",
    "\n",
    "---\n",
    "\n",
    "In summary, content-based filtering is a strong baseline for recommendation, especially when:\n",
    "- item metadata is rich,\n",
    "- user preferences are diverse,\n",
    "- and we want transparent and controllable recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d646ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "DATA_PATH = 'data_final_project/KuaiRec 2.0/data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3970fd",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ef399",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "The recommendation system is based on several CSV files that represent user interactions and video metadata:\n",
    "\n",
    "---\n",
    "\n",
    "#### `big_matrix.csv`\n",
    "- Contains historical **user-video interactions**.\n",
    "- Used for **training** user profiles.\n",
    "- Includes fields like: `user_id`, `video_id`, `watch_ratio`, `play_duration`, etc.\n",
    "\n",
    "#### `small_matrix.csv`\n",
    "- Contains **test interactions**, recorded after the training period.\n",
    "- Used to **evaluate** recommendation quality.\n",
    "\n",
    "---\n",
    "\n",
    "#### `item_daily_features.csv`\n",
    "- Provides **daily aggregated statistics per video**, such as:\n",
    "  - `play_cnt`, `like_cnt`, `comment_cnt`, `complete_play_cnt`, etc.\n",
    "- Used to compute **engagement metrics** and **popularity scores**.\n",
    "\n",
    "#### `item_categories.csv`\n",
    "- Contains **categorical tags** per video.\n",
    "- Field `feat` is a list of semantic categories.\n",
    "- Used for **one-hot encoding** of video types or genres.\n",
    "\n",
    "---\n",
    "\n",
    "#### `kuairec_caption_category.csv`\n",
    "- Includes **textual metadata** per video:\n",
    "  - `caption`, `manual_cover_text`, and topic-level categories.\n",
    "- Used to extract **TF-IDF features** from captions for semantic modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1f9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading datasets...\")\n",
    "\t\t\n",
    "# Load interaction data\n",
    "interactions_train = pd.read_csv(os.path.join(DATA_PATH, \"big_matrix.csv\"))\n",
    "interactions_test = pd.read_csv(os.path.join(DATA_PATH, \"small_matrix.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f0bee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video metadata\n",
    "item_categories = pd.read_csv(os.path.join(DATA_PATH, \"item_categories.csv\"))\n",
    "kuairec_caption = pd.read_csv(\n",
    "    os.path.join(DATA_PATH, \"kuairec_caption_category.csv\"),\n",
    "    engine=\"python\", sep=\",\", quotechar='\"', on_bad_lines='skip'\n",
    ")\n",
    "item_daily_features = pd.read_csv(os.path.join(DATA_PATH, \"item_daily_features.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74e5ba5",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a081f2fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "# Show differences before and after dropna and drop_duplicates\n",
    "\n",
    "def show_diff(df, name):\n",
    "    before = df.shape[0]\n",
    "    after = df.dropna().drop_duplicates().shape[0]\n",
    "    print(f\"{name}: {before} rows -> {after} rows ({before - after} removed)\")\n",
    "\n",
    "show_diff(interactions_train, \"interactions_train\")\n",
    "show_diff(interactions_test, \"interactions_test\")\n",
    "show_diff(item_categories, \"item_categories\")\n",
    "show_diff(item_daily_features, \"item_daily_features\")\n",
    "show_diff(kuairec_caption, \"kuairec_caption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74998456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique video IDs in kuairec_caption: 10728\n",
      "Positive interactions in train set: 5683062 / 12530806 (45.35%)\n",
      "Positive interactions in test set: 2218724 / 4676570 (47.44%)\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Remove bad ids in kuairec_caption_category\n",
    "kuairec_caption = kuairec_caption[pd.to_numeric(kuairec_caption['video_id'], errors='coerce').notna()]\n",
    "kuairec_caption['video_id'] = kuairec_caption['video_id'].astype(int)\n",
    "print(f\"Number of unique video IDs in kuairec_caption: {kuairec_caption['video_id'].nunique()}\")\n",
    "\n",
    "\n",
    "# Process watch_ratio - clamp extreme values to keep them in a reasonable range\n",
    "# Normally, a watch_ratio > 1 can indicate replays, but clamp to 2.0 as a reasonable max value\n",
    "interactions_train['watch_ratio_clamped'] = np.clip(interactions_train['watch_ratio'], 0, 2.0)\n",
    "interactions_test['watch_ratio_clamped'] = np.clip(interactions_test['watch_ratio'], 0, 2.0)\n",
    "\n",
    "# Adjust the positive interaction threshold based on data analysis\n",
    "POSITIVE_THRESHOLD = 0.8\n",
    "interactions_train['positive_interaction'] = (interactions_train['watch_ratio_clamped'] >= POSITIVE_THRESHOLD).astype(int)\n",
    "interactions_test['positive_interaction'] = (interactions_test['watch_ratio_clamped'] >= POSITIVE_THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"Positive interactions in train set: {interactions_train['positive_interaction'].sum()} / {len(interactions_train)} ({interactions_train['positive_interaction'].mean()*100:.2f}%)\")\n",
    "print(f\"Positive interactions in test set: {interactions_test['positive_interaction'].sum()} / {len(interactions_test)} ({interactions_test['positive_interaction'].mean()*100:.2f}%)\")\n",
    "\n",
    "print(\"Data loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb566f",
   "metadata": {},
   "source": [
    "### Extracting Video Features\n",
    "\n",
    "The function `extract_video_features()` builds a **comprehensive feature representation** for each video by combining:\n",
    "\n",
    "- aggregated engagement statistics,\n",
    "- category tags (one-hot encoding),\n",
    "- and semantic text features (TF-IDF over captions).\n",
    "\n",
    "These features are used to compute **content-based similarities** between users and videos.\n",
    "\n",
    "---\n",
    "\n",
    "#### Main Steps:\n",
    "\n",
    "1. ### **Engagement Statistics (from `item_daily_features`)**\n",
    "   For each video, compute aggregated metrics over time:\n",
    "   - `play_cnt`, `like_cnt`, `comment_cnt`, `share_cnt`, etc.\n",
    "   - Derived ratios such as `like_ratio`, `comment_ratio`, `completion_ratio`, etc.\n",
    "   - A `popularity_score` based on log-scaled raw counts\n",
    "   - An `engagement_score` based on the mean of behavioral ratios\n",
    "\n",
    "   All numeric features are later **standardized (z-score)** to normalize scale.\n",
    "\n",
    "2. ### **Category Tags (from `item_categories`)**\n",
    "   - Convert list of tags stored as strings into actual Python lists\n",
    "   - Extract all unique categories\n",
    "   - One-hot encode the presence of each category per video\n",
    "\n",
    "   This creates a **sparse categorical feature vector** per video, representing its semantic type.\n",
    "\n",
    "3. ### **Caption Text Features (from `kuairec_caption`)**\n",
    "   - Clean and collect captions per video\n",
    "   - Extract TF-IDF features (300-dimension) using `TfidfVectorizer`\n",
    "   - Each vector captures the semantic content of the video's caption\n",
    "\n",
    "4. ### **Merging & Normalization**\n",
    "   - Merge all features into a single dataframe by `video_id`\n",
    "   - Fill missing values with 0 (for videos missing categories or captions)\n",
    "   - Normalize the final feature matrix with `StandardScaler`\n",
    "   - Create a lookup map from `video_id` to row index\n",
    "\n",
    "---\n",
    "\n",
    "#### Output:\n",
    "\n",
    "- `video_features`: Pandas DataFrame with all raw features per video\n",
    "- `video_features_matrix`: Numpy matrix (normalized) of shape `(n_videos, n_features)`\n",
    "- `video_id_map`: Dictionary mapping `video_id` → row index in `video_features_matrix`\n",
    "\n",
    "---\n",
    "\n",
    "#### Why it matters:\n",
    "\n",
    "This step encodes **rich content information** for every video, enabling:\n",
    "- similarity comparisons via cosine distance,\n",
    "- user profile construction via feature aggregation,\n",
    "- interpretability (engagement scores, textual signals, tags).\n",
    "\n",
    "It is a **core component** for any content-based filtering system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b5d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting video features...\n",
      "Extracted features for 10728 videos with 349 features each\n",
      "Video features extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "def extract_video_features():\n",
    "    \"\"\"\n",
    "    Extract and process features from video metadata to create feature vector for each video.\n",
    "    \"\"\"\n",
    "    print(\"Extracting video features...\")\n",
    "    \n",
    "    # 1. Extract video metrics from item_daily_features\n",
    "    video_metrics = item_daily_features.groupby('video_id').agg({\n",
    "        'video_duration': 'mean',\n",
    "        'play_cnt': 'sum',\n",
    "        'like_cnt': 'sum',\n",
    "        'comment_cnt': 'sum',\n",
    "        'share_cnt': 'sum',\n",
    "        'follow_cnt': 'sum',\n",
    "        'complete_play_cnt': 'sum',\n",
    "        'valid_play_cnt': 'sum',\n",
    "        'play_user_num': 'max',\n",
    "        'like_user_num': 'max'\n",
    "    }).reset_index()\n",
    "    \n",
    "    video_metrics['like_ratio'] = video_metrics['like_cnt'] / video_metrics['play_cnt'].clip(lower=1)\n",
    "    video_metrics['comment_ratio'] = video_metrics['comment_cnt'] / video_metrics['play_cnt'].clip(lower=1)\n",
    "    video_metrics['share_ratio'] = video_metrics['share_cnt'] / video_metrics['play_cnt'].clip(lower=1)\n",
    "    video_metrics['follow_ratio'] = video_metrics['follow_cnt'] / video_metrics['play_cnt'].clip(lower=1)\n",
    "    video_metrics['completion_ratio'] = video_metrics['complete_play_cnt'] / video_metrics['play_cnt'].clip(lower=1)\n",
    "    video_metrics['validity_ratio'] = video_metrics['valid_play_cnt'] / video_metrics['play_cnt'].clip(lower=1)\n",
    "    \n",
    "    # MinMaxScaler is used here to scale the computed popularity and engagement scores to a [0, 1] range.\n",
    "    # To ensure the features are on the same scale as other normalized features,\n",
    "    video_metrics['popularity_score'] = MinMaxScaler().fit_transform(\n",
    "        np.log1p(video_metrics[['play_cnt', 'like_cnt', 'comment_cnt', 'share_cnt']]).sum(axis=1).values.reshape(-1, 1)\n",
    "    )\n",
    "    video_metrics['engagement_score'] = MinMaxScaler().fit_transform(\n",
    "        (video_metrics[['like_ratio', 'comment_ratio', 'share_ratio', 'follow_ratio']].mean(axis=1)).values.reshape(-1, 1)\n",
    "    )\n",
    "\n",
    "    # 2. Extract category information from item_categories\n",
    "    # Convert string representation of list to actual list\n",
    "    item_categories['feat'] = item_categories['feat'].apply(lambda x: json.loads(x.replace(\"'\", '\"')) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Create one-hot encoding for categories\n",
    "    category_features = pd.DataFrame(item_categories['video_id'])\n",
    "\n",
    "    # Get all unique categories\n",
    "    all_categories = set()\n",
    "    for categories in item_categories['feat']:\n",
    "        if isinstance(categories, list):\n",
    "            all_categories.update(categories)\n",
    "    \n",
    "    # Initialize category columns with zeros\n",
    "    for category in all_categories:\n",
    "        category_features[f'category_{category}'] = 0\n",
    "        \n",
    "    # Fill in category values\n",
    "    for idx, row in item_categories.iterrows():\n",
    "        if isinstance(row['feat'], list):\n",
    "            for category in row['feat']:\n",
    "                category_features.loc[idx, f'category_{category}'] = 1\n",
    "\n",
    "\n",
    "    # 3. Extract content features from kuairec_caption\n",
    "    caption_df = kuairec_caption.copy()\n",
    "    caption_df = caption_df[['video_id', 'caption']].dropna()\n",
    "    caption_df['caption'] = caption_df['caption'].astype(str)\n",
    "\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=300,\n",
    "    )\n",
    "\n",
    "    tfidf_matrix = tfidf.fit_transform(caption_df['caption'])\n",
    "\n",
    "    tfidf_features_df = pd.DataFrame(\n",
    "        tfidf_matrix.toarray(),\n",
    "        index=caption_df['video_id'].values,\n",
    "        columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])]\n",
    "    ).reset_index().rename(columns={'index': 'video_id'})\n",
    "\n",
    "    # 4. Merge all features\n",
    "    video_features = video_metrics.merge(category_features, on='video_id', how='left')\n",
    "    video_features = video_features.merge(tfidf_features_df, on='video_id', how='left')\n",
    "    \n",
    "    video_features = video_features.fillna(0)\n",
    "    \n",
    "    # Create a mapping from video_id to index for quick lookup\n",
    "    video_id_map = {video_id: idx for idx, video_id in enumerate(video_features['video_id'])}\n",
    "    \n",
    "    feature_matrix = video_features.drop('video_id', axis=1)\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features = scaler.fit_transform(feature_matrix)\n",
    "    \n",
    "    video_features_matrix = normalized_features\n",
    "    \n",
    "    print(f\"Extracted features for {len(video_features)} videos with {video_features_matrix.shape[1]} features each\")\n",
    "\n",
    "    return video_features, video_features_matrix, video_id_map\n",
    "\n",
    "video_features, video_features_matrix, video_id_map = extract_video_features()\n",
    "print(\"Video features extracted successfully!\")\n",
    "display(video_features)\n",
    "display(video_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2719a80",
   "metadata": {},
   "source": [
    "### Building User Profiles\n",
    "\n",
    "The function `build_user_profiles()` constructs a **profile vector for each user** based on their **positive interactions** with videos. This profile represents the user's preferences in the same feature space as the videos, enabling personalized content-based recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "#### How It Works:\n",
    "\n",
    "1. **Filter positive interactions**:\n",
    "   - The function selects only interactions with a `watch_ratio_clamped` ≥ threshold (typically 0.7), marking them as positive.\n",
    "\n",
    "2. **Initialize user profile matrix**:\n",
    "   - For each unique user in the training set, an empty profile vector is initialized.\n",
    "\n",
    "3. **Aggregate liked video features**:\n",
    "   - For every user:\n",
    "     - It collects all videos with which the user had a positive interaction.\n",
    "     - For each such video, its feature vector (from `video_features_matrix`) is retrieved.\n",
    "     - Each vector is weighted by the `watch_ratio` to reflect how strongly the user engaged with the content.\n",
    "\n",
    "4. **Compute weighted average**:\n",
    "   - The user’s profile is the **weighted average** of all liked video vectors, weighted by watch ratio.\n",
    "   - This captures not only which videos were liked, but also **how much** they were liked.\n",
    "\n",
    "5. **Normalize the user profile**:\n",
    "   - To ensure consistent cosine similarity comparisons, each user profile is normalized to unit length.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why it matters:\n",
    "\n",
    "- This approach builds a **compact representation** of a user's preferences directly in the feature space of the videos.\n",
    "- It enables efficient **cosine similarity** matching with unseen videos to rank and recommend the most relevant content.\n",
    "- Weighting by `watch_ratio` allows the system to prioritize stronger preferences in the user’s history.\n",
    "\n",
    "---\n",
    "\n",
    "#### Output:\n",
    "\n",
    "- `user_profiles`: NumPy array of shape `(n_users, n_features)`, storing profile vectors.\n",
    "- `user_id_map`: Dictionary mapping user IDs to row indices in `user_profiles`.\n",
    "- `unique_users`: Array of user IDs corresponding to the profile matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building user profiles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building user profiles: 100%|██████████| 7176/7176 [05:12<00:00, 22.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built profiles for 7176 users\n",
      "User profiles built successfully!\n"
     ]
    }
   ],
   "source": [
    "def build_user_profiles():\n",
    "    \"\"\"\n",
    "    Build user profiles based on their positive interactions with videos.\n",
    "    Each user profile is a weighted average of the features of videos they liked.\n",
    "    \"\"\"\n",
    "    print(\"Building user profiles...\")\n",
    "    \n",
    "    # Get positive interactions only\n",
    "    positive_interactions = interactions_train[interactions_train['positive_interaction'] == 1]\n",
    "    unique_users = positive_interactions['user_id'].unique()\n",
    "    \n",
    "    # Create a mapping from user_id to index for quick lookup\n",
    "    user_id_map = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "    \n",
    "    user_profiles = np.zeros((len(unique_users), video_features_matrix.shape[1]))\n",
    "    \n",
    "    # Build user profiles by aggregating the features of videos they liked\n",
    "    for user_id in tqdm(unique_users, desc=\"Building user profiles\"):\n",
    "        # Get all positive interactions for this user\n",
    "        user_interactions = positive_interactions[positive_interactions['user_id'] == user_id]\n",
    "        \n",
    "        if len(user_interactions) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get the features of videos this user liked\n",
    "        user_video_features = []\n",
    "        user_video_weights = []\n",
    "        \n",
    "        for _, interaction in user_interactions.iterrows():\n",
    "            video_id = interaction['video_id']\n",
    "            watch_ratio = interaction['watch_ratio']  # Use watch_ratio as weight\n",
    "            \n",
    "            if video_id in video_id_map:\n",
    "                video_idx = video_id_map[video_id]\n",
    "                user_video_features.append(video_features_matrix[video_idx])\n",
    "                user_video_weights.append(watch_ratio)\n",
    "        \n",
    "        if user_video_features:\n",
    "            user_video_features = np.array(user_video_features)\n",
    "            user_video_weights = np.array(user_video_weights)\n",
    "            \n",
    "            # Normalize weights\n",
    "            user_video_weights = user_video_weights / np.sum(user_video_weights)\n",
    "            \n",
    "            # Compute weighted average of video features\n",
    "            user_profile = np.average(user_video_features, axis=0, weights=user_video_weights)\n",
    "\n",
    "            # Normalize user profile\n",
    "            user_profile /= np.linalg.norm(user_profile) if np.linalg.norm(user_profile) > 0 else 1.0\n",
    "\n",
    "            user_idx = user_id_map[user_id]\n",
    "            user_profiles[user_idx] = user_profile\n",
    "    \n",
    "    print(f\"Built profiles for {len(unique_users)} users\")\n",
    "    return user_profiles, user_id_map, unique_users\n",
    "\n",
    "user_profiles, user_id_map, unique_users = build_user_profiles()\n",
    "print(\"User profiles built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201f710",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 119\u001b[0m\n\u001b[1;32m    114\u001b[0m     plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseen_unseen_distribution_user_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43munique_users\u001b[49m[:\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    120\u001b[0m     recommendations \u001b[38;5;241m=\u001b[39m generate_recommendations(user_id, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    121\u001b[0m     plot_similarity_scores(user_id, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_users' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_recommendations(user_id, top_n=10, exclude_watched=True):\n",
    "    \"\"\"\n",
    "    Generate recommendations for a user using cosine similarity between\n",
    "    the user profile and video features.\n",
    "    \"\"\"\n",
    "    if user_id not in user_id_map:\n",
    "        print(f\"User {user_id} not found in training data\")\n",
    "        return []\n",
    "    \n",
    "    user_idx = user_id_map[user_id]\n",
    "    user_profile = user_profiles[user_idx].reshape(1, -1)\n",
    "    \n",
    "    # Calculate similarity between user profile and all videos\n",
    "    similarities = cosine_similarity(user_profile, video_features_matrix)[0]\n",
    "    \n",
    "    video_ids = video_features['video_id'].values\n",
    "    video_similarities = list(zip(video_ids, similarities))\n",
    "    \n",
    "    # Sort by similarity in descending order\n",
    "    video_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if exclude_watched:\n",
    "        # Get list of videos the user has already watched\n",
    "        watched_videos = set(interactions_train[\n",
    "            interactions_train['user_id'] == user_id\n",
    "        ]['video_id'].values)\n",
    "        \n",
    "        # Filter out watched videos\n",
    "        video_similarities = [(vid, sim) for vid, sim in video_similarities if vid not in watched_videos]\n",
    "    \n",
    "    # Return top N recommended video IDs\n",
    "    top_recommendations = [(vid, sim) for vid, sim in video_similarities[:top_n]]\n",
    "    return top_recommendations\n",
    "\n",
    "def plot_similarity_scores(user_id, top_n=10):\n",
    "    \"\"\"\n",
    "    Plot similarity scores between a user's profile and all video feature vectors.\n",
    "    \"\"\"\n",
    "    if user_id not in user_id_map:\n",
    "        print(f\"User {user_id} not found in training data\")\n",
    "        return\n",
    "    \n",
    "    user_idx = user_id_map[user_id]\n",
    "    user_profile = user_profiles[user_idx].reshape(1, -1)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = cosine_similarity(user_profile, video_features_matrix)[0]\n",
    "    video_ids = video_features['video_id'].values\n",
    "\n",
    "    # Get watched videos\n",
    "    watched_videos = set(interactions_train[\n",
    "        interactions_train['user_id'] == user_id\n",
    "    ]['video_id'].values)\n",
    "\n",
    "    # Build DataFrame\n",
    "    similarity_df = pd.DataFrame({\n",
    "        'video_id': video_ids,\n",
    "        'similarity': similarities,\n",
    "        'watched': [vid in watched_videos for vid in video_ids]\n",
    "    })\n",
    "\n",
    "    # Sort to highlight top N recommendations\n",
    "    similarity_df_sorted = similarity_df.sort_values(by='similarity', ascending=False).reset_index(drop=True)\n",
    "    top_recommended = similarity_df_sorted.head(top_n)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(similarity_df['similarity'], bins=50, kde=True, label='All videos', color='skyblue')\n",
    "    plt.axvline(top_recommended['similarity'].min(), color='red', linestyle='--', label=f'Top-{top_n} threshold')\n",
    "    plt.title(f\"Similarity Score Distribution (user_id={user_id})\")\n",
    "    plt.xlabel(\"Similarity Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"similarity_distribution_user_{user_id}.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_similarity_seen_vs_unseen(user_id):\n",
    "    \"\"\"\n",
    "    Compare similarity scores of videos seen vs unseen by the user.\n",
    "    \"\"\"\n",
    "    if user_id not in user_id_map:\n",
    "        print(f\"User {user_id} not found.\")\n",
    "        return\n",
    "    \n",
    "    user_idx = user_id_map[user_id]\n",
    "    user_profile = user_profiles[user_idx].reshape(1, -1)\n",
    "    \n",
    "    # Similarity between user profile and all videos\n",
    "    similarities = cosine_similarity(user_profile, video_features_matrix)[0]\n",
    "    video_ids = video_features['video_id'].values\n",
    "    \n",
    "    watched_videos = set(\n",
    "        interactions_train[interactions_train['user_id'] == user_id]['video_id'].values\n",
    "    )\n",
    "    \n",
    "    # Create a DataFrame with similarities and watched/unwatched status\n",
    "    df = pd.DataFrame({\n",
    "        'video_id': video_ids,\n",
    "        'similarity': similarities,\n",
    "        'watched': [vid in watched_videos for vid in video_ids]\n",
    "    })\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[df['watched']]['similarity'], bins=50, kde=True, color='green', label='Watched', stat='density')\n",
    "    sns.histplot(df[~df['watched']]['similarity'], bins=50, kde=True, color='gray', label='Unwatched', stat='density')\n",
    "    \n",
    "    plt.title(f\"Similarity Score Distribution (user_id={user_id})\\nWatched vs Unwatched Videos\")\n",
    "    plt.xlabel(\"Similarity Score\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"seen_unseen_distribution_user_{user_id}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "for user_id in unique_users[:1]:\n",
    "    recommendations = generate_recommendations(user_id, top_n=10)\n",
    "    plot_similarity_scores(user_id, top_n=20)\n",
    "    plot_similarity_seen_vs_unseen(user_id)\n",
    "    print(f\"Recommendations for user {user_id}: {recommendations}\")\n",
    "\n",
    "recommendations = generate_recommendations(5733, top_n=10)\n",
    "plot_similarity_scores(5733, top_n=20)\n",
    "plot_similarity_seen_vs_unseen(5733)\n",
    "print(f\"Recommendations for user {5733}: {recommendations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18477b64",
   "metadata": {},
   "source": [
    "### Evaluation: Ranking Quality on Test Videos\n",
    "\n",
    "The function `evaluate_model()` assesses how well the content-based recommender system ranks videos that the user has **actually seen** in the test set (`small_matrix`). It does this by computing standard ranking metrics at top-K.\n",
    "\n",
    "---\n",
    "\n",
    "#### Evaluation Setup:\n",
    "\n",
    "- **Users evaluated**: All users present in the test set (`small_matrix`) and whose profiles were built from the training set (`big_matrix`).\n",
    "- **Candidate set**: Only videos that each user has seen in the test set.\n",
    "- **Ground truth**: Videos with a `watch_ratio_clamped` above a certain threshold (e.g., `0.7`) are considered **positive interactions** (i.e., liked).\n",
    "- **Predictions**: Videos are ranked using **cosine similarity** between the user profile and the video feature vectors.\n",
    "\n",
    "---\n",
    "\n",
    "#### Metrics Computed:\n",
    "\n",
    "For each user, the following metrics are computed at `k` (typically 5, 10, or 20):\n",
    "\n",
    "| Metric        | Definition                                                                 |\n",
    "|---------------|---------------------------------------------------------------------------|\n",
    "| `Precision@k` | Fraction of top-k recommended videos that are truly liked                 |\n",
    "| `Recall@k`    | Fraction of all liked videos that appear in the top-k                     |\n",
    "| `NDCG@k`      | Discounted cumulative gain, rewarding relevant items near the top of the list |\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This Evaluation Matters\n",
    "\n",
    "- This method avoids unfairly evaluating on **unseen videos** by restricting ranking to the subset the user has actually seen.\n",
    "- It gives a realistic view of how well the system **ranks known items**, which is particularly useful in **offline validation**.\n",
    "- It highlights how the system prioritizes **truly preferred** content over neutral or low-engagement content.\n",
    "\n",
    "---\n",
    "\n",
    "#### Output\n",
    "\n",
    "The function returns a dictionary:\n",
    "\n",
    "```python\n",
    "{\n",
    "  'precision@k': float,\n",
    "  'recall@k': float,\n",
    "  'ndcg@k': float,\n",
    "  'users_evaluated': int\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc15797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation par classement sur les vidéos du test... (top-10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Utilisateurs évalués:   0%|          | 0/1411 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Utilisateurs évalués: 100%|██████████| 1411/1411 [02:06<00:00, 11.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisateurs évalués : 1411 / 1411\n",
      "Utilisateurs ignorés : 0\n",
      "Precision@10: 0.8147\n",
      "Recall@10:    0.0053\n",
      "NDCG@10:      0.9222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(k=10, positive_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Evaluate the ranking quality of already seen videos by the user (in small_matrix),\n",
    "    using their profile learned from big_matrix.\n",
    "    \"\"\"\n",
    "    print(f\"Ranking evaluation on test videos... (top-{k})\")\n",
    "\n",
    "    # List of users to evaluate = those present in small_matrix + known users\n",
    "    test_users = interactions_test['user_id'].unique()\n",
    "    test_users = [u for u in test_users if u in user_id_map]\n",
    "\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    ndcg_list = []\n",
    "    skipped = 0\n",
    "\n",
    "    for user_id in tqdm(test_users, desc=\"Evaluating users\"):\n",
    "\n",
    "        user_idx = user_id_map[user_id]\n",
    "        user_profile = user_profiles[user_idx].reshape(1, -1)\n",
    "\n",
    "        # Videos this user has seen in small_matrix\n",
    "        user_test_data = interactions_test[\n",
    "            (interactions_test['user_id'] == user_id) &\n",
    "            (interactions_test['video_id'].isin(video_id_map))\n",
    "        ]\n",
    "\n",
    "        if user_test_data.empty:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        candidate_videos = user_test_data['video_id'].unique()\n",
    "\n",
    "        # Ground truth: liked videos\n",
    "        positive_videos = set(\n",
    "            user_test_data[user_test_data['watch_ratio_clamped'] >= positive_threshold]['video_id']\n",
    "        )\n",
    "\n",
    "        if len(positive_videos) == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # Compute similarity between user profile and candidate videos\n",
    "        candidate_indices = [video_id_map[vid] for vid in candidate_videos]\n",
    "        candidate_features = video_features_matrix[candidate_indices]\n",
    "        similarities = cosine_similarity(user_profile, candidate_features)[0]\n",
    "\n",
    "        # Associate each video with its similarity\n",
    "        ranked = sorted(zip(candidate_videos, similarities), key=lambda x: x[1], reverse=True)\n",
    "        ranked_video_ids = [vid for vid, _ in ranked[:k]]\n",
    "\n",
    "        # Evaluation\n",
    "        recommended_set = set(ranked_video_ids)\n",
    "        intersection = positive_videos & recommended_set\n",
    "\n",
    "        precision = len(intersection) / k\n",
    "        recall = len(intersection) / len(positive_videos)\n",
    "        relevance = [1 if vid in positive_videos else 0 for vid in ranked_video_ids]\n",
    "        ndcg = ndcg_score([relevance], [list(range(k, 0, -1))])\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        ndcg_list.append(ndcg)\n",
    "\n",
    "    print(f\"Users evaluated: {len(precision_list)} / {len(test_users)}\")\n",
    "    print(f\"Users skipped: {skipped}\")\n",
    "\n",
    "    return {\n",
    "        'precision@k': np.mean(precision_list),\n",
    "        'recall@k': np.mean(recall_list),\n",
    "        'ndcg@k': np.mean(ndcg_list),\n",
    "        'users_evaluated': len(precision_list)\n",
    "    }\n",
    "\n",
    "results = evaluate_model(k=10, positive_threshold=0.8)\n",
    "print(f\"Precision@10: {results['precision@k']:.4f}\")\n",
    "print(f\"Recall@10:    {results['recall@k']:.4f}\")\n",
    "print(f\"NDCG@10:      {results['ndcg@k']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db80d5e",
   "metadata": {},
   "source": [
    "### Evaluation Results — Ranking on Test Set (Top-10)\n",
    "\n",
    "The model was evaluated by ranking videos already seen by users in the test set (`small_matrix`). Only users whose profiles were available from the training set (`big_matrix`) were considered.\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "- **Users evaluated**: 1411\n",
    "- **Users skipped**: 0  \n",
    "- **Evaluation threshold**: `watch_ratio_clamped ≥ 0.7` (positive interaction)\n",
    "- **Top-K cutoff**: `k = 10`\n",
    "\n",
    "| Metric        | Value   | Interpretation |\n",
    "|---------------|---------|----------------|\n",
    "| `Precision@10`| 0.8147  | Among the top 10 ranked videos per user, ~81% were truly liked (high quality of ranking) |\n",
    "| `Recall@10`   | 0.0053  | Only 0.5% of all liked videos were recovered in the top-10 (due to high number of positives) |\n",
    "| `NDCG@10`     | 0.9222  | Excellent ranking quality — relevant items appear very early in the list |\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **High precision (0.81)**: The top-10 ranked videos for each user are highly aligned with their preferences. This indicates that user profiles and video feature vectors are **well-aligned** in the embedding space.\n",
    "\n",
    "- **Very low recall (0.0053)**: Despite high precision, the proportion of all truly liked videos recovered is very small. This is expected given:\n",
    "  - Users have **hundreds or thousands of positives** in the test set (average ~1850),\n",
    "  - The top-K list contains only **10 predictions**.\n",
    "\n",
    "- **High NDCG (0.92)**: The model is not only predicting liked videos, but is ranking them **very well** — relevant videos appear early in the list, which is crucial for recommendation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc1d662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Évaluation par classement sur les vidéos du test... (top-5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Utilisateurs évalués: 100%|██████████| 1411/1411 [02:09<00:00, 10.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisateurs évalués : 1411 / 1411\n",
      "Utilisateurs ignorés : 0\n",
      "Precision@5: 0.8719\n",
      "Recall@5:    0.0024\n",
      "NDCG@5:      0.9454\n",
      "Évaluation par classement sur les vidéos du test... (top-20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Utilisateurs évalués: 100%|██████████| 1411/1411 [02:07<00:00, 11.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisateurs évalués : 1411 / 1411\n",
      "Utilisateurs ignorés : 0\n",
      "Precision@20: 0.8637\n",
      "Recall@20:    0.0094\n",
      "NDCG@20:      0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(k=5)\n",
    "print(f\"Precision@5: {results['precision@k']:.4f}\")\n",
    "print(f\"Recall@5:    {results['recall@k']:.4f}\")\n",
    "print(f\"NDCG@5:      {results['ndcg@k']:.4f}\")\n",
    "\n",
    "results = evaluate_model(k=20)\n",
    "print(f\"Precision@20: {results['precision@k']:.4f}\")\n",
    "print(f\"Recall@20:    {results['recall@k']:.4f}\")\n",
    "print(f\"NDCG@20:      {results['ndcg@k']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df21de18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_users' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43munique_users\u001b[49m[:\u001b[38;5;241m5\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     recommendations \u001b[38;5;241m=\u001b[39m generate_recommendations(user_id, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommendations for user \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecommendations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique_users' is not defined"
     ]
    }
   ],
   "source": [
    "for user_id in unique_users[:10]:\n",
    "    recommendations = generate_recommendations(user_id, top_n=10)\n",
    "    print(f\"Recommendations for user {user_id}: {recommendations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender-system-lectures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
