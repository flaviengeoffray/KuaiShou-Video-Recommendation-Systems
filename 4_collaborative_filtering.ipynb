{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c66afaf",
   "metadata": {},
   "source": [
    "### Collaborative Filtering: Overview and Motivation\n",
    "\n",
    "**Collaborative filtering** is a recommendation approach that leverages the **collective preferences and behaviors of users** to make personalized recommendations. Unlike content-based filtering which focuses on item features, collaborative filtering relies on patterns of user-item interactions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Choose Collaborative Filtering?\n",
    "\n",
    "- **Discovers latent patterns**: Can detect preferences and similarities that aren't captured in explicit item features.\n",
    "- **No content analysis required**: Works without needing detailed feature engineering of items.\n",
    "- **Serendipity**: Can recommend unexpected but relevant items beyond a user's typical content profile.\n",
    "- **Community wisdom**: Leverages the collective intelligence of the user base.\n",
    "- **Domain agnostic**: Can work across different types of content without domain-specific feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "#### Approaches to Collaborative Filtering\n",
    "\n",
    "1. **Memory-Based Approaches**\n",
    "   - **User-User**: Recommends items liked by users similar to the target user.\n",
    "   - **Item-Item**: Recommends items similar to those the user already liked.\n",
    "\n",
    "2. **Model-Based Approaches**\n",
    "   - **Matrix Factorization**: Decomposes user-item interaction matrix into latent factors.\n",
    "   - **Neural Networks**: Uses deep learning to model complex user-item interactions.\n",
    "\n",
    "---\n",
    "\n",
    "#### Limitations (to be addressed in hybrid systems)\n",
    "\n",
    "- **Cold-start problem**: Difficult to recommend to new users or items with few interactions.\n",
    "- **Sparsity issues**: Most users interact with only a small fraction of available items.\n",
    "- **Popularity bias**: Tendency to recommend popular items over niche content.\n",
    "\n",
    "---\n",
    "\n",
    "In summary, collaborative filtering excels when:\n",
    "- large interaction datasets are available,\n",
    "- understanding the social/community dynamics of consumption is important,\n",
    "- and discovering non-obvious recommendations is valued.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b62ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ndcg_score\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "DATA_PATH = 'data_final_project/KuaiRec 2.0/data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d0c1c",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13185df",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "The recommendation system is based on several CSV files that represent user interactions and video metadata:\n",
    "\n",
    "---\n",
    "\n",
    "#### `big_matrix.csv`\n",
    "- Contains historical **user-video interactions**.\n",
    "- Used for **training** user profiles.\n",
    "- Includes fields like: `user_id`, `video_id`, `watch_ratio`, `play_duration`, etc.\n",
    "\n",
    "#### `small_matrix.csv`\n",
    "- Contains **test interactions**, recorded after the training period.\n",
    "- Used to **evaluate** recommendation quality.\n",
    "\n",
    "---\n",
    "\n",
    "#### `item_daily_features.csv`\n",
    "- Provides **daily aggregated statistics per video**.\n",
    "- Used for additional analysis but not directly in collaborative filtering.\n",
    "\n",
    "#### `item_categories.csv` and `kuairec_caption_category.csv`\n",
    "- Contains metadata that may be used for hybrid approaches but are not central to pure collaborative filtering.\n",
    "\n",
    "---\n",
    "\n",
    "For collaborative filtering, we primarily focus on the **interaction matrices** (`big_matrix.csv` and `small_matrix.csv`), as the approach relies on patterns of user-item interactions rather than content features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2129b76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Training interactions: (12530806, 8)\n",
      "Test interactions: (4676570, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "\t\t\n",
    "# Load interaction data\n",
    "interactions_train = pd.read_csv(os.path.join(DATA_PATH, \"big_matrix.csv\"))\n",
    "interactions_test = pd.read_csv(os.path.join(DATA_PATH, \"small_matrix.csv\"))\n",
    "\n",
    "# Load video metadata (may be used in hybrid approaches or analysis)\n",
    "item_daily_features = pd.read_csv(os.path.join(DATA_PATH, \"item_daily_features.csv\"))\n",
    "\n",
    "print(f\"Training interactions: {interactions_train.shape}\")\n",
    "print(f\"Test interactions: {interactions_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495a4e0",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e2b85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interactions_train: 12530806 rows -> 11564987 rows (965819 removed)\n",
      "interactions_test: 4676570 rows -> 4494578 rows (181992 removed)\n",
      "item_daily_features: 343341 rows -> 239968 rows (103373 removed)\n"
     ]
    }
   ],
   "source": [
    "# Show differences before and after dropna and drop_duplicates\n",
    "\n",
    "def preprocess(df, name):\n",
    "    \"\"\"Remove missing values and duplicates, and report cleaning stats\"\"\"\n",
    "    before = df.shape[0]\n",
    "    after = df.dropna().drop_duplicates().shape[0]\n",
    "    print(f\"{name}: {before} rows -> {after} rows ({before - after} removed)\")\n",
    "    return df.dropna().drop_duplicates()\n",
    "\n",
    "interactions_train = preprocess(interactions_train, \"interactions_train\")\n",
    "interactions_test = preprocess(interactions_test, \"interactions_test\")\n",
    "item_daily_features = preprocess(item_daily_features, \"item_daily_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786fc838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive interactions in train set: 5966291 / 11564987 (51.59%)\n",
      "Positive interactions in test set: 2536880 / 4494578 (56.44%)\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Process watch_ratio - clamp extreme values to keep them in a reasonable range\n",
    "# Normally, a watch_ratio > 1 can indicate replays, but clamp to 2.0 as a reasonable max value\n",
    "interactions_train['watch_ratio_clamped'] = np.clip(interactions_train['watch_ratio'], 0, 2.0)\n",
    "interactions_test['watch_ratio_clamped'] = np.clip(interactions_test['watch_ratio'], 0, 2.0)\n",
    "\n",
    "# Adjust the positive interaction threshold based on data analysis\n",
    "POSITIVE_THRESHOLD = 0.7\n",
    "interactions_train['positive_interaction'] = (interactions_train['watch_ratio_clamped'] >= POSITIVE_THRESHOLD).astype(int)\n",
    "interactions_test['positive_interaction'] = (interactions_test['watch_ratio_clamped'] >= POSITIVE_THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"Positive interactions in train set: {interactions_train['positive_interaction'].sum()} / {len(interactions_train)} ({interactions_train['positive_interaction'].mean()*100:.2f}%)\")\n",
    "print(f\"Positive interactions in test set: {interactions_test['positive_interaction'].sum()} / {len(interactions_test)} ({interactions_test['positive_interaction'].mean()*100:.2f}%)\")\n",
    "\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222417d",
   "metadata": {},
   "source": [
    "### Building User-Item Interaction Matrices\n",
    "\n",
    "For collaborative filtering, we need to construct **user-item matrices** that represent user interactions with videos. We'll create two types of matrices:\n",
    "\n",
    "1. **Binary Matrix**: Where entries are 1 if the user had a positive interaction with the video (watch_ratio â‰¥ 0.7) and 0 otherwise.\n",
    "2. **Rating Matrix**: Where entries are the actual watch_ratio values, representing the strength of interest.\n",
    "\n",
    "These matrices will be used for user-user similarity, item-item similarity, and matrix factorization approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69528aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user-item interaction matrices...\n",
      "Matrix dimensions: 7176 users x 10728 videos\n",
      "Binary matrix: 10300969 non-zero entries (density: 0.133806)\n",
      "Rating matrix: 10300969 non-zero entries (density: 0.133806)\n"
     ]
    }
   ],
   "source": [
    "def create_interaction_matrices():\n",
    "    \"\"\"Create user-item interaction matrices for collaborative filtering\"\"\"\n",
    "    print(\"Creating user-item interaction matrices...\")\n",
    "    \n",
    "    # Create unique user and item mappings\n",
    "    unique_users = np.union1d(interactions_train['user_id'].unique(), interactions_test['user_id'].unique())\n",
    "    unique_videos = np.union1d(interactions_train['video_id'].unique(), interactions_test['video_id'].unique())\n",
    "    \n",
    "    user_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "    video_to_idx = {video_id: idx for idx, video_id in enumerate(unique_videos)}\n",
    "    idx_to_user = {idx: user_id for user_id, idx in user_to_idx.items()}\n",
    "    idx_to_video = {idx: video_id for video_id, idx in video_to_idx.items()}\n",
    "    \n",
    "    # Calculate matrix dimensions\n",
    "    n_users = len(unique_users)\n",
    "    n_videos = len(unique_videos)\n",
    "    print(f\"Matrix dimensions: {n_users} users x {n_videos} videos\")\n",
    "    \n",
    "    # Create sparse matrices for training data\n",
    "    train_rows = interactions_train['user_id'].map(user_to_idx).values\n",
    "    train_cols = interactions_train['video_id'].map(video_to_idx).values\n",
    "    \n",
    "    # Binary matrix (positive interactions)\n",
    "    binary_data = interactions_train['positive_interaction'].values\n",
    "    binary_matrix = csr_matrix((binary_data, (train_rows, train_cols)), shape=(n_users, n_videos))\n",
    "    \n",
    "    # Rating matrix (watch_ratio values)\n",
    "    rating_data = interactions_train['watch_ratio_clamped'].values\n",
    "    rating_matrix = csr_matrix((rating_data, (train_rows, train_cols)), shape=(n_users, n_videos))\n",
    "    \n",
    "    print(f\"Binary matrix: {binary_matrix.nnz} non-zero entries (density: {binary_matrix.nnz / (n_users * n_videos):.6f})\")\n",
    "    print(f\"Rating matrix: {rating_matrix.nnz} non-zero entries (density: {rating_matrix.nnz / (n_users * n_videos):.6f})\")\n",
    "    \n",
    "    return binary_matrix, rating_matrix, user_to_idx, video_to_idx, idx_to_user, idx_to_video\n",
    "\n",
    "binary_matrix, rating_matrix, user_to_idx, video_to_idx, idx_to_user, idx_to_video = create_interaction_matrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f955957",
   "metadata": {},
   "source": [
    "### User-User Collaborative Filtering\n",
    "\n",
    "User-User collaborative filtering works by finding users with similar preferences and recommending items that these similar users have enjoyed.\n",
    "\n",
    "Steps:\n",
    "1. Calculate similarity between users based on their interaction patterns\n",
    "2. For each user, identify the most similar users\n",
    "3. Recommend items that similar users have liked but the target user hasn't seen yet\n",
    "\n",
    "This approach is intuitive but can be computationally expensive with large user bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15cbc896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-User CF recommendations for user 14:\n",
      "Video 3282: Score 11.7344\n",
      "Video 10021: Score 11.1498\n",
      "Video 8247: Score 11.1180\n",
      "Video 6951: Score 11.0990\n",
      "Video 4694: Score 10.8222\n",
      "Video 5789: Score 10.5506\n",
      "Video 2845: Score 10.2379\n",
      "Video 8328: Score 9.9632\n",
      "Video 9717: Score 9.9502\n",
      "Video 7779: Score 9.6383\n"
     ]
    }
   ],
   "source": [
    "def user_user_collaborative_filtering(user_id, n_neighbors=50, top_n=10):\n",
    "    \"\"\"Generate recommendations using user-user collaborative filtering\"\"\"\n",
    "    if user_id not in user_to_idx:\n",
    "        print(f\"User {user_id} not found in the dataset\")\n",
    "        return []\n",
    "    \n",
    "    user_idx = user_to_idx[user_id]\n",
    "    user_profile = binary_matrix[user_idx].toarray().flatten()\n",
    "    \n",
    "    # Find videos this user has already interacted with\n",
    "    user_videos = set(np.where(user_profile > 0)[0])\n",
    "    if len(user_videos) == 0:\n",
    "        print(f\"User {user_id} has no interactions in the training set\")\n",
    "        return []\n",
    "    \n",
    "    # Calculate similarity with other users who have at least one interaction\n",
    "    active_users = np.where(binary_matrix.getnnz(axis=1) > 0)[0]\n",
    "    similarities = []\n",
    "    \n",
    "    # Use a batch approach for large matrices\n",
    "    for u_idx in active_users:\n",
    "        if u_idx == user_idx:\n",
    "            continue\n",
    "        other_profile = binary_matrix[u_idx].toarray().flatten()\n",
    "        sim = cosine_similarity(user_profile.reshape(1, -1), other_profile.reshape(1, -1))[0][0]\n",
    "        if sim > 0:  # Only consider positive similarity\n",
    "            similarities.append((u_idx, sim))\n",
    "    \n",
    "    # Sort by similarity and get top neighbors\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    neighbors = similarities[:n_neighbors]\n",
    "    \n",
    "    # Calculate score for each video based on similar users' interactions\n",
    "    video_scores = {}\n",
    "    for neighbor_idx, similarity in neighbors:\n",
    "        neighbor_videos = set(np.where(binary_matrix[neighbor_idx].toarray().flatten() > 0)[0])\n",
    "        for vid_idx in neighbor_videos:\n",
    "            if vid_idx not in user_videos:  # Only recommend videos the user hasn't seen\n",
    "                if vid_idx not in video_scores:\n",
    "                    video_scores[vid_idx] = 0\n",
    "                # Weight the video by the similarity score\n",
    "                video_scores[vid_idx] += similarity\n",
    "    \n",
    "    # Sort videos by score and get top_n recommendations\n",
    "    sorted_videos = sorted(video_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    recommendations = [(idx_to_video[vid_idx], score) for vid_idx, score in sorted_videos]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "test_user_id = interactions_test['user_id'].iloc[0]\n",
    "user_user_recommendations = user_user_collaborative_filtering(test_user_id, n_neighbors=50, top_n=10)\n",
    "print(f\"User-User CF recommendations for user {test_user_id}:\")\n",
    "for video_id, score in user_user_recommendations:\n",
    "    print(f\"Video {video_id}: Score {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf67bac",
   "metadata": {},
   "source": [
    "### Item-Item Collaborative Filtering\n",
    "\n",
    "Item-Item collaborative filtering recommends items similar to those the user has already liked. It's often more scalable than user-user CF since:\n",
    "- The item catalog changes less frequently than user preferences\n",
    "- Item similarity matrices can be pre-computed\n",
    "\n",
    "Steps:\n",
    "1. Calculate similarity between items based on user interaction patterns\n",
    "2. For each item a user has liked, find similar items\n",
    "3. Aggregate these similar items to create personalized recommendations\n",
    "\n",
    "This approach is widely used in production systems due to its efficiency and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf5d63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating item similarities...\n",
      "Computing similarities for 5000 most popular items...\n",
      "Item similarity matrix shape: (5000, 5000)\n"
     ]
    }
   ],
   "source": [
    "def calculate_item_similarities(n_items=5000):\n",
    "    \"\"\"Calculate similarity between items based on user interaction patterns\"\"\"\n",
    "    print(\"Calculating item similarities...\")\n",
    "    \n",
    "    # For efficiency, we'll only calculate similarities for the most interacted-with items\n",
    "    item_interaction_counts = binary_matrix.sum(axis=0).A1\n",
    "    top_items_idx = np.argsort(item_interaction_counts)[-n_items:]\n",
    "    \n",
    "    # Create a map of video indices to their positions in the similarity matrix\n",
    "    item_pos_map = {idx: pos for pos, idx in enumerate(top_items_idx)}\n",
    "    \n",
    "    # Extract submatrix with only top items\n",
    "    top_items_matrix = binary_matrix[:, top_items_idx].transpose()\n",
    "    \n",
    "    print(f\"Computing similarities for {n_items} most popular items...\")\n",
    "    item_similarity = cosine_similarity(top_items_matrix)\n",
    "    \n",
    "    # Set self-similarity to 0 to avoid recommending the same item\n",
    "    np.fill_diagonal(item_similarity, 0)\n",
    "    \n",
    "    print(f\"Item similarity matrix shape: {item_similarity.shape}\")\n",
    "    return item_similarity, item_pos_map, top_items_idx\n",
    "\n",
    "# Calculate similarities for the most popular items (adjust n_items as needed)\n",
    "item_similarity, item_pos_map, top_items_idx = calculate_item_similarities(n_items=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3a14e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-Item CF recommendations for user 14:\n",
      "Video 4694: Score 62.5350\n",
      "Video 3282: Score 62.3537\n",
      "Video 8328: Score 59.6561\n",
      "Video 6951: Score 58.4308\n",
      "Video 8247: Score 58.4220\n",
      "Video 10021: Score 58.0466\n",
      "Video 2123: Score 57.4027\n",
      "Video 8435: Score 56.9225\n",
      "Video 4751: Score 56.4862\n",
      "Video 2638: Score 56.4165\n"
     ]
    }
   ],
   "source": [
    "def item_item_collaborative_filtering(user_id, top_n=10):\n",
    "    \"\"\"Generate recommendations using item-item collaborative filtering\"\"\"\n",
    "    if user_id not in user_to_idx:\n",
    "        print(f\"User {user_id} not found in the dataset\")\n",
    "        return []\n",
    "    \n",
    "    user_idx = user_to_idx[user_id]\n",
    "    user_profile = binary_matrix[user_idx].toarray().flatten()\n",
    "    \n",
    "    # Find items this user has already interacted with\n",
    "    user_items = np.where(user_profile > 0)[0]\n",
    "    if len(user_items) == 0:\n",
    "        print(f\"User {user_id} has no interactions in the training set\")\n",
    "        return []\n",
    "    \n",
    "    # Calculate recommendation scores for all items\n",
    "    scores = np.zeros(binary_matrix.shape[1])\n",
    "    for item_idx in user_items:\n",
    "        # Skip if item is not in the top items for which we calculated similarity\n",
    "        if item_idx not in item_pos_map:\n",
    "            continue\n",
    "            \n",
    "        # Get similarities to this item and update scores\n",
    "        item_pos = item_pos_map[item_idx]\n",
    "        similar_items = item_similarity[item_pos]\n",
    "        \n",
    "        for pos, sim in enumerate(similar_items):\n",
    "            if sim > 0:  # Only consider positive similarity\n",
    "                comparable_item_idx = top_items_idx[pos]\n",
    "                scores[comparable_item_idx] += sim\n",
    "    \n",
    "    # Zero out items the user has already interacted with\n",
    "    scores[user_items] = 0\n",
    "    \n",
    "    # Get top N recommendations\n",
    "    top_item_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "    recommendations = [(idx_to_video[idx], scores[idx]) for idx in top_item_indices if scores[idx] > 0]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "item_item_recommendations = item_item_collaborative_filtering(test_user_id, top_n=10)\n",
    "print(f\"Item-Item CF recommendations for user {test_user_id}:\")\n",
    "for video_id, score in item_item_recommendations:\n",
    "    print(f\"Video {video_id}: Score {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c1ed1",
   "metadata": {},
   "source": [
    "### Matrix Factorization Approach\n",
    "\n",
    "Matrix Factorization is a model-based collaborative filtering approach that decomposes the user-item interaction matrix into lower-dimensional latent factor matrices. These latent factors can capture hidden patterns in user preferences and item characteristics.\n",
    "\n",
    "Key benefits:\n",
    "- Can handle sparsity better than memory-based approaches\n",
    "- Often provides better prediction accuracy\n",
    "- Reduces dimensionality for computational efficiency\n",
    "- Can capture latent factors not evident in raw data\n",
    "\n",
    "We'll implement Singular Value Decomposition (SVD) to factorize the rating matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eed90d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing matrix factorization with 50 latent factors...\n",
      "Matrix factorization completed\n"
     ]
    }
   ],
   "source": [
    "def matrix_factorization(n_factors=50):\n",
    "    \"\"\"Perform matrix factorization using Singular Value Decomposition (SVD)\"\"\"\n",
    "    print(f\"Performing matrix factorization with {n_factors} latent factors...\")\n",
    "    \n",
    "    # Convert sparse matrix to dense (this could be memory-intensive for large matrices)\n",
    "    # For production systems, consider using libraries like surprise or implicit that handle sparse matrices directly\n",
    "    matrix = rating_matrix.toarray()\n",
    "    \n",
    "    # Fill missing values with column means (average rating for each item)\n",
    "    # In production, more sophisticated imputation methods could be used\n",
    "    item_means = np.nanmean(matrix, axis=0)\n",
    "    item_means[np.isnan(item_means)] = 0  # Handle items with no ratings\n",
    "    matrix_filled = matrix.copy()\n",
    "    \n",
    "    for col in range(matrix.shape[1]):\n",
    "        mask = matrix[:, col] == 0\n",
    "        matrix_filled[mask, col] = item_means[col]\n",
    "    \n",
    "    # Perform SVD\n",
    "    U, sigma, Vt = svds(matrix_filled, k=n_factors)\n",
    "    sigma = np.diag(sigma)\n",
    "    \n",
    "    # Predict ratings: U Ã— sigma Ã— Vt\n",
    "    predicted_ratings = np.dot(np.dot(U, sigma), Vt)\n",
    "    \n",
    "    print(\"Matrix factorization completed\")\n",
    "    return predicted_ratings\n",
    "\n",
    "# Generate the predicted ratings matrix\n",
    "predicted_ratings = matrix_factorization(n_factors=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55b91c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Factorization recommendations for user 14:\n",
      "Video 3400: Score 1.5975\n",
      "Video 4751: Score 1.4310\n",
      "Video 4694: Score 1.3057\n",
      "Video 8328: Score 1.2772\n",
      "Video 8435: Score 1.2699\n",
      "Video 6951: Score 1.2257\n",
      "Video 2845: Score 1.2134\n",
      "Video 3871: Score 1.1290\n",
      "Video 9758: Score 1.1099\n",
      "Video 3827: Score 1.1028\n"
     ]
    }
   ],
   "source": [
    "def matrix_factorization_recommendations(user_id, top_n=10):\n",
    "    \"\"\"Generate recommendations using matrix factorization results\"\"\"\n",
    "    if user_id not in user_to_idx:\n",
    "        print(f\"User {user_id} not found in the dataset\")\n",
    "        return []\n",
    "    \n",
    "    user_idx = user_to_idx[user_id]\n",
    "    user_interactions = binary_matrix[user_idx].toarray().flatten()\n",
    "    user_predictions = predicted_ratings[user_idx]\n",
    "    \n",
    "    # Set already watched items to negative infinity to exclude them\n",
    "    watched_items = np.where(user_interactions > 0)[0]\n",
    "    user_predictions[watched_items] = float('-inf')\n",
    "    \n",
    "    # Get indices of top N predictions\n",
    "    top_indices = np.argsort(user_predictions)[-top_n:][::-1]\n",
    "    \n",
    "    # Create recommendations list with scores\n",
    "    recommendations = [(idx_to_video[idx], user_predictions[idx]) for idx in top_indices]\n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "mf_recommendations = matrix_factorization_recommendations(test_user_id, top_n=10)\n",
    "print(f\"Matrix Factorization recommendations for user {test_user_id}:\")\n",
    "for video_id, score in mf_recommendations:\n",
    "    print(f\"Video {video_id}: Score {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0722295f",
   "metadata": {},
   "source": [
    "### Evaluation Framework\n",
    "\n",
    "We'll evaluate our collaborative filtering approaches using the same metrics as in the content-based notebook:\n",
    "- Precision@k: Proportion of recommended items that were actually relevant\n",
    "- Recall@k: Proportion of relevant items that were successfully recommended\n",
    "- NDCG@k: Normalized Discounted Cumulative Gain, which measures ranking quality\n",
    "\n",
    "This will allow direct comparison between collaborative and content-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f773bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommender(recommendation_func, k=10, positive_threshold=0.7):\n",
    "    \"\"\"Evaluate a recommender function on test data\"\"\"\n",
    "    print(f\"Evaluating recommender on test videos... (top-{k})\")\n",
    "    \n",
    "    # List of users to evaluate = those present in small_matrix + known users\n",
    "    test_users = interactions_test['user_id'].unique()\n",
    "    test_users = [u for u in test_users if u in user_to_idx]\n",
    "    \n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    ndcg_list = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for user_id in tqdm(test_users[:100], desc=\"Evaluating users\"):  # Limit to 100 users for speed\n",
    "        # Videos this user has seen in small_matrix\n",
    "        user_test_data = interactions_test[\n",
    "            (interactions_test['user_id'] == user_id) &\n",
    "            (interactions_test['video_id'].isin(video_to_idx))\n",
    "        ]\n",
    "        \n",
    "        if user_test_data.empty:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Ground truth: liked videos in test set\n",
    "        positive_videos = set(\n",
    "            user_test_data[user_test_data['watch_ratio_clamped'] >= positive_threshold]['video_id']\n",
    "        )\n",
    "        \n",
    "        if len(positive_videos) == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = recommendation_func(user_id, top_n=k)\n",
    "        if not recommendations:\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        recommended_videos = [video_id for video_id, _ in recommendations]\n",
    "        \n",
    "        # Evaluation\n",
    "        recommended_set = set(recommended_videos)\n",
    "        intersection = positive_videos & recommended_set\n",
    "        \n",
    "        precision = len(intersection) / k\n",
    "        recall = len(intersection) / len(positive_videos)\n",
    "        relevance = [1 if vid in positive_videos else 0 for vid in recommended_videos]\n",
    "        ndcg = ndcg_score([relevance], [list(range(k, 0, -1))])\n",
    "        \n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        ndcg_list.append(ndcg)\n",
    "    \n",
    "    print(f\"Users evaluated: {len(precision_list)} / {len(test_users[:100])}\")\n",
    "    print(f\"Users skipped: {skipped}\")\n",
    "    \n",
    "    return {\n",
    "        'precision@k': np.mean(precision_list) if precision_list else 0,\n",
    "        'recall@k': np.mean(recall_list) if recall_list else 0,\n",
    "        'ndcg@k': np.mean(ndcg_list) if ndcg_list else 0,\n",
    "        'users_evaluated': len(precision_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9136b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recommender on test videos... (top-5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:29<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users evaluated: 100 / 100\n",
      "Users skipped: 0\n",
      "\n",
      "Item-Item Collaborative Filtering Results (k=5):\n",
      "Precision@5: 0.0660\n",
      "Recall@5:    0.0002\n",
      "NDCG@5:      0.0984\n",
      "Evaluating recommender on test videos... (top-5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 48.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users evaluated: 100 / 100\n",
      "Users skipped: 0\n",
      "\n",
      "Matrix Factorization Results (k=5):\n",
      "Precision@5: 0.2800\n",
      "Recall@5:    0.0008\n",
      "NDCG@5:      0.4603\n",
      "Evaluating recommender on test videos... (top-10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:30<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users evaluated: 100 / 100\n",
      "Users skipped: 0\n",
      "\n",
      "Item-Item Collaborative Filtering Results (k=10):\n",
      "Precision@10: 0.1720\n",
      "Recall@10:    0.0009\n",
      "NDCG@10:      0.3221\n",
      "Evaluating recommender on test videos... (top-10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 49.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users evaluated: 100 / 100\n",
      "Users skipped: 0\n",
      "\n",
      "Matrix Factorization Results (k=10):\n",
      "Precision@10: 0.3390\n",
      "Recall@10:    0.0020\n",
      "NDCG@10:      0.5479\n",
      "Evaluating recommender on test videos... (top-20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:30<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users evaluated: 100 / 100\n",
      "Users skipped: 0\n",
      "\n",
      "Item-Item Collaborative Filtering Results (k=20):\n",
      "Precision@20: 0.3225\n",
      "Recall@20:    0.0035\n",
      "NDCG@20:      0.4860\n",
      "Evaluating recommender on test videos... (top-20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00, 48.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users evaluated: 100 / 100\n",
      "Users skipped: 0\n",
      "\n",
      "Matrix Factorization Results (k=20):\n",
      "Precision@20: 0.4205\n",
      "Recall@20:    0.0048\n",
      "NDCG@20:      0.6273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [5, 10, 20]:\n",
    "    # Evaluate Item-Item CF\n",
    "    ii_results = evaluate_recommender(item_item_collaborative_filtering, k=k)\n",
    "    print(f\"\\nItem-Item Collaborative Filtering Results (k={k}):\")\n",
    "    print(f\"Precision@{k}: {ii_results['precision@k']:.4f}\")\n",
    "    print(f\"Recall@{k}:    {ii_results['recall@k']:.4f}\")\n",
    "    print(f\"NDCG@{k}:      {ii_results['ndcg@k']:.4f}\")\n",
    "\n",
    "    # Evaluate Matrix Factorization\n",
    "    mf_results = evaluate_recommender(matrix_factorization_recommendations, k=k)\n",
    "    print(f\"\\nMatrix Factorization Results (k={k}):\")\n",
    "    print(f\"Precision@{k}: {mf_results['precision@k']:.4f}\")\n",
    "    print(f\"Recall@{k}:    {mf_results['recall@k']:.4f}\")\n",
    "    print(f\"NDCG@{k}:      {mf_results['ndcg@k']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
